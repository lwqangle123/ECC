{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading citeseer dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deeprobust/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n",
      "/anaconda3/envs/deeprobust/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/anaconda3/envs/deeprobust/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 1 largest connected components\n"
     ]
    }
   ],
   "source": [
    "from deeprobust.graph.data import Dataset\n",
    "\n",
    "from deeprobust.graph.utils import get_train_val_test\n",
    "# loading cora dataset\n",
    "data = Dataset(root='/tmp/', name='citeseer', seed=15)\n",
    "adj, features, labels = data.adj, data.features, data.labels\n",
    "# you can also split the data by yourself\n",
    "# idx_train, idx_val, idx_test = get_train_val_test(adj.shape[0], val_size=0.1, test_size=0.8)\n",
    "\n",
    "# loading acm dataset\n",
    "#data = Dataset(root='/tmp/', name='acm', seed=15)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "#np.random.bit_generator = np.random._bit_generator\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from deeprobust.graph.defense import GCN\n",
    "from deeprobust.graph.defense import GAT\n",
    "from deeprobust.graph.targeted_attack import RND\n",
    "from deeprobust.graph.global_attack import DICE\n",
    "from deeprobust.graph.utils import *\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
    "idx_train, idx_val, idx_test = get_train_val_test(adj.shape[0], val_size=0.2, test_size=0.2, stratify=data.labels)\n",
    "\n",
    "\n",
    "idx_unlabeled = np.union1d(idx_val, idx_test)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeprobust.graph.global_attack import PGDAttack\n",
    "adj, features, labels = preprocess(adj, features, labels, preprocess_adj=False) \n",
    "gcn = GCN(nfeat=features.shape[1], nclass=labels.max().item()+1,\n",
    "                        nhid=16, dropout=0.5, weight_decay=5e-4, device='cpu').to('cpu')\n",
    "gcn.fit(features, adj, labels, idx_train)\n",
    "\n",
    "data.idx_train = idx_train\n",
    "data.idx_val = idx_val\n",
    "data.idx_test = idx_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topology Attack\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "N=10\n",
    "M_adj = torch.zeros((1,np.shape(adj)[0]*np.shape(adj)[1]))\n",
    "for i in range(N):\n",
    "    model = PGDAttack(model=gcn, nnodes=adj.shape[0], loss_type='CE', device='cpu').to('cpu')\n",
    "    B = 0.01 + np.zeros((np.shape(adj)[0],np.shape(adj)[0]))\n",
    "    adj_n = np.logical_xor(adj, np.random.binomial(1,B)).float()+torch.zeros((np.shape(adj)[0],np.shape(adj)[0]))\n",
    "    adj_nn = torch.triu(adj_n,diagonal=1)\n",
    "    adj_noise = adj_nn.T+adj_nn\n",
    "    model.attack(features, adj_noise, labels, idx_train, n_perturbations=100000)\n",
    "    M_adj = np.vstack((M_adj,model.modified_adj.reshape(1,np.shape(adj)[0]*np.shape(adj)[1])))\n",
    "\n",
    "MM_adj = M_adj[1:,:]\n",
    "R_adj=np.int64(sum(MM_adj)/N>0.5)\n",
    "Rr_adj=R_adj.reshape(np.shape(adj)[0],np.shape(adj)[1])\n",
    "np.shape(np.where((Rr_adj-np.int64(adj))!=0))[1]/(np.shape(adj)[0]*np.shape(adj)[1])#recover error\n",
    "\n",
    "O1=gcn.predict(features, Rr_adj).argmax(1)\n",
    "O2=gcn.predict(features, adj).argmax(1)\n",
    "np.shape(np.where(O1!=O2))[1]#misclassification rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DICE Attack\n",
    "N=10\n",
    "M_adj = np.zeros((1,np.shape(adj)[0]*np.shape(adj)[1]))\n",
    "adj_mm = np.zeros((np.shape(adj)[0],np.shape(adj)[1]))\n",
    "for i in range(N):\n",
    "    model = DICE()\n",
    "    B = 0.01 + np.zeros((np.shape(adj)[0],np.shape(adj)[0]))\n",
    "    adj_n = np.logical_xor(adj, np.random.binomial(1,B)).float()+torch.zeros((np.shape(adj)[0],np.shape(adj)[0]))\n",
    "    adj_nn = torch.triu(adj_n,diagonal=1)\n",
    "    adj_noise = adj_nn.T+adj_nn\n",
    "    adj_nnn = sp.csr_matrix(adj_noise).tocsr()\n",
    "    model.attack(adj_nnn, labels, n_perturbations=100000)\n",
    "    modified_adj=model.modified_adj\n",
    "    for ii in range(np.shape(modified_adj)[0]):\n",
    "                a = modified_adj[ii].rows           \n",
    "                for jj in range(len(a[0])):\n",
    "                    adj_mm[ii,a[0][jj]]=1    \n",
    "    M_adj = np.vstack((M_adj,adj_mm.reshape(1,np.shape(adj)[0]*np.shape(adj)[1])))\n",
    "\n",
    "MM_adj = M_adj[1:,:]\n",
    "R_adj=np.int64(sum(MM_adj)/N>0.5)\n",
    "Rr_adj=R_adj.reshape(np.shape(adj)[0],np.shape(adj)[1])\n",
    "np.shape(np.where((Rr_adj-np.int64(adj))!=0))[1]/(np.shape(adj)[0]*np.shape(adj)[1])#recover error\n",
    "\n",
    "O1=gcn.predict(features, Rr_adj).argmax(1)\n",
    "O2=gcn.predict(features, adj).argmax(1)\n",
    "np.shape(np.where(O1!=O2))[1]#misclassification rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "620\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1428\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "620\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1388\n",
      "tensor(4)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1428\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1905\n",
      "tensor(4)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1554\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "620\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1905\n",
      "tensor(4)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1453\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1388\n",
      "tensor(4)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "645\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1018\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "896\n",
      "tensor(4)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "268\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1699\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "2062\n",
      "tensor(4)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "2084\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1453\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1859\n",
      "tensor(4)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "688\n",
      "tensor(4)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "953\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "181\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1905\n",
      "tensor(4)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1905\n",
      "tensor(4)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "2084\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "953\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1699\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1790\n",
      "tensor(4)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "987\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "220\n",
      "tensor(4)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "452\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1716\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1400\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "896\n",
      "tensor(4)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1067\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "620\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "2062\n",
      "tensor(4)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "904\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "987\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1905\n",
      "tensor(4)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "114\n",
      "tensor(4)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "220\n",
      "tensor(4)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "662\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "645\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "448\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "1716\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "80\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "2090\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "number of pertubations: 1000\n",
      "662\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "#Random Attack\n",
    "j=1\n",
    "Out_1 =np.array([])\n",
    "Out_2 =np.array([])\n",
    "while(j<=1000):# 1000 target nodes\n",
    "    M_adj = torch.zeros((1,np.shape(adj)[0]*np.shape(adj)[1]))\n",
    "    degrees = adj.sum(0)\n",
    "    k = np.random.randint(0,np.shape(idx_unlabeled)[0])\n",
    "    target_node = idx_unlabeled[k]\n",
    "    if int(degrees[target_node])<10:\n",
    "        continue\n",
    "    adj_mat = np.zeros((np.shape(adj)[0],np.shape(adj)[1]))\n",
    "    for i in range(10):#repreat number 10\n",
    "        model = RND()\n",
    "        B = 0.0001 + np.zeros((np.shape(adj)[0],np.shape(adj)[0]))\n",
    "        adj_n = np.logical_xor(adj, np.random.binomial(1,B)).float()+torch.zeros((np.shape(adj)[0],np.shape(adj)[0]))\n",
    "        adj_nn = torch.triu(adj_n,diagonal=1)\n",
    "        adj_noise = adj_nn.T+adj_nn\n",
    "        adj_nnn = sp.csr_matrix(adj_noise).tocsr()\n",
    "        model.attack(adj_nnn, labels, idx_train, target_node, n_perturbations=100)\n",
    "        for ii in range(np.shape(adj)[0]):\n",
    "            a = model.modified_adj[ii].rows\n",
    "            for jj in range(len(a[0])):\n",
    "                adj_mat[ii,a[0][jj]]=1\n",
    "        M_adj = np.vstack((M_adj,adj_mat.reshape(1,np.shape(adj)[0]*np.shape(adj)[1])))\n",
    "    MM_adj = M_adj[1:,:]\n",
    "    R_adj=np.int64(sum(MM_adj)/10>0.5)\n",
    "    Rr_adj=R_adj.reshape(np.shape(adj)[0],np.shape(adj)[1])\n",
    "    np.shape(np.where((Rr_adj-np.int64(adj))!=0))[1]/(np.shape(adj)[0]*np.shape(adj)[1])#recover error\n",
    "    O1=gcn.predict(features, Rr_adj).argmax(1)[target_node]\n",
    "    O2=gcn.predict(features, adj).argmax(1)[target_node]\n",
    "    Out_1=np.hstack((Out_1,O1))\n",
    "    Out_2=np.hstack((Out_2,O2))\n",
    "    j+=1\n",
    "np.shape(np.where(Out_1-Out_2!=0))[1]/1000 #mr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pertubations: 8\n",
      "(3025, 1870)\n",
      "tensor([[-2.0488, -0.5839, -1.1602],\n",
      "        [-1.5482, -0.6928, -1.2476],\n",
      "        [-1.6864, -0.6142, -1.2956],\n",
      "        ...,\n",
      "        [-1.6784, -0.7289, -1.1060],\n",
      "        [-1.8741, -0.5094, -1.4039],\n",
      "        [-1.2342, -0.8546, -1.2606]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# train surrogate model\n",
    "degrees = adj.sum(0).A1\n",
    "n_perturbations = int(degrees[u]) # How many perturbations to perform. Default: Degree of the node\n",
    "#model = RND()\n",
    "#model.attack(adj, labels, idx_train, target_node, n_perturbations, verbose=False)\n",
    "\n",
    "\n",
    "# modified_adj = model.attack(adj, labels, idx_train, target_node, n_perturbations)\n",
    "model.add_nodes(features, adj, labels, idx_train, target_node, n_added=0,\n",
    "                               n_perturbations=n_perturbations)\n",
    "modified_adj = model.modified_adj\n",
    "modified_features = model.modified_features\n",
    "#modified_features = features\n",
    "gcn = GCN(nfeat=features.shape[1],\n",
    "                  nhid=16,\n",
    "                 nclass=labels.max().item() + 1,\n",
    "                  dropout=0.5, device=device)\n",
    "\n",
    "gcn = gcn.to(device)\n",
    "output = gcn.predict(modified_features, modified_adj)\n",
    "\n",
    "print(np.shape(modified_features))\n",
    "print(output)\n",
    "\n",
    "def test(adj, features, target_node):\n",
    "    ''' test on GCN '''\n",
    "    gcn = GCN(nfeat=features.shape[1],\n",
    "              nhid=16,\n",
    "              nclass=labels.max().item() + 1,\n",
    "              dropout=0.5, device=device)\n",
    "\n",
    "    gcn = gcn.to(device)\n",
    "\n",
    "    gcn.fit(features, adj, labels, idx_train)\n",
    "    gcn.eval()\n",
    "    output = gcn.predict()\n",
    "    probs = torch.exp(output[[target_node]])[0]\n",
    "    print('probs: {probs.detach().cpu().numpy()}')\n",
    "\n",
    "    labels_tensor = torch.LongTensor(labels).to(device)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels_tensor[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels_tensor[idx_test])\n",
    "\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "    return acc_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_test(adj, features, target_node, gcn=None):\n",
    "    if gcn is None:\n",
    "        # test on GCN (poisoning attack)\n",
    "        gcn = GCN(nfeat=features.shape[1],\n",
    "                  nhid=16,\n",
    "                  nclass=labels.max().item() + 1,\n",
    "                  dropout=0.5, device=device)\n",
    "\n",
    "        gcn = gcn.to(device)\n",
    "\n",
    "        gcn.fit(features, adj, labels, idx_train, idx_val, patience=30)\n",
    "        gcn.eval()\n",
    "        output = gcn.predict()\n",
    "    else:\n",
    "        # test on GCN (evasion attack)\n",
    "        output = gcn.predict(features, adj)\n",
    "    probs = torch.exp(output[[target_node]])\n",
    "    #print(output)\n",
    "    #print(output.argmax(1))\n",
    "    #print(output.argmax(1)[target_node], labels[target_node])\n",
    "\n",
    "    # acc_test = accuracy(output[[target_node]], labels[target_node])\n",
    "    acc_test = (output.argmax(1)[target_node] == labels[target_node])\n",
    "    return acc_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_nodes(target_gcn=None):\n",
    "    '''\n",
    "    selecting nodes as reported in nettack paper:\n",
    "    (i) the 10 nodes with highest margin of classification, i.e. they are clearly correctly classified,\n",
    "    (ii) the 10 nodes with lowest margin (but still correctly classified) and\n",
    "    (iii) 20 more nodes randomly\n",
    "    '''\n",
    "\n",
    "    if target_gcn is None:\n",
    "        target_gcn = GCN(nfeat=features.shape[1],\n",
    "                  nhid=16,\n",
    "                  nclass=labels.max().item() + 1,\n",
    "                  dropout=0.5, device=device)\n",
    "        target_gcn = target_gcn.to(device)\n",
    "        target_gcn.fit(features, adj, labels, idx_train, idx_val, patience=30)\n",
    "    target_gcn.eval()\n",
    "    output = target_gcn.predict()\n",
    "\n",
    "    margin_dict = {}\n",
    "    for idx in idx_test:\n",
    "        margin = classification_margin(output[idx], labels[idx])\n",
    "        if margin < 0: # only keep the nodes correctly classified\n",
    "            continue\n",
    "        margin_dict[idx] = margin\n",
    "    sorted_margins = sorted(margin_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "    high = [x for x, y in sorted_margins[: 10]]\n",
    "    low = [x for x, y in sorted_margins[-10: ]]\n",
    "    other = [x for x, y in sorted_margins[10: -10]]\n",
    "    other = np.random.choice(other, 20, replace=False).tolist()\n",
    "\n",
    "    return high + low + other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_test_evasion():\n",
    "    # test on 40 nodes on evasion attack\n",
    "    target_gcn = GCN(nfeat=features.shape[1],\n",
    "              nhid=16,\n",
    "              nclass=labels.max().item() + 1,\n",
    "              dropout=0.5, device=device)\n",
    "\n",
    "    target_gcn = target_gcn.to(device)\n",
    "\n",
    "    target_gcn.fit(features, adj, labels, idx_train, idx_val, patience=30)\n",
    "\n",
    "    cnt = 0\n",
    "    degrees = adj.sum(0).A1\n",
    "    node_list = select_nodes(target_gcn)\n",
    "    num = len(node_list)\n",
    "\n",
    "    print('=== [Evasion] Attacking %s nodes respectively ===' % num)\n",
    "    for target_node in tqdm(node_list):\n",
    "        n_perturbations = int(degrees[target_node])\n",
    "        model = RND()\n",
    "        model = model.to(device)\n",
    "        model.attack(adj, labels, idx_train, target_node, n_perturbations, verbose=False)\n",
    "        model.add_nodes(features, adj, labels, idx_train, target_node, n_added=40,\n",
    "                               n_perturbations=n_perturbations)\n",
    "        modified_adj = model.modified_adj\n",
    "        modified_features = model.modified_features\n",
    "        #print(modified_adj)\n",
    "        print(modified_features)\n",
    "        acc = single_test(modified_adj, modified_features, target_node, gcn=target_gcn)\n",
    "        if acc == 0:\n",
    "            cnt += 1\n",
    "    print('misclassification rate : %s' % (cnt/num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print('=== testing GCN on original(clean) graph ===')\n",
    "    test(adj, features, target_node)\n",
    "    print('=== testing GCN on perturbed graph ===')\n",
    "    if modified_features is None:\n",
    "        test(modified_adj, features, target_node)\n",
    "    else:\n",
    "        test(modified_adj, modified_features, target_node)\n",
    "\n",
    "    return modified_features\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== testing GCN on original(clean) graph ===\n",
      "probs: {probs.detach().cpu().numpy()}\n",
      "Test set results: loss= 1.3053 accuracy= 0.7227\n",
      "=== testing GCN on perturbed graph ===\n",
      "probs: {probs.detach().cpu().numpy()}\n",
      "Test set results: loss= 1.2632 accuracy= 0.7145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    mf = main()\n",
    "   # multi_test_evasion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
